AWSTemplateFormatVersion: '2010-09-09'
Description: 'Test Infrastructure for Inspector Validation Lambda v2 - Complete Lambda from original template with L1/L2 changes'

Parameters:
  Environment:
    Type: String
    Default: test
    AllowedValues:
      - test
      - dev
    Description: Environment name

  pSuppressionRuleDynamodbTableName:
    Type: String
    Default: inspector-suppression-rules-test
    Description: DynamoDB table for suppression rules

  pSuppressionEngineCacheDBName:
    Type: String
    Default: inspector-suppression-cache-test
    Description: DynamoDB table for cache

Resources:
  # ============================================================================
  # DynamoDB Tables
  # ============================================================================
  SuppressionRulesTable:
    Type: AWS::DynamoDB::Table
    Properties:
      TableName: !Ref pSuppressionRuleDynamodbTableName
      BillingMode: PAY_PER_REQUEST
      AttributeDefinitions:
        - AttributeName: id
          AttributeType: S
        - AttributeName: ser_id
          AttributeType: S
      KeySchema:
        - AttributeName: id
          KeyType: HASH
        - AttributeName: ser_id
          KeyType: RANGE

  CacheTable:
    Type: AWS::DynamoDB::Table
    Properties:
      TableName: !Ref pSuppressionEngineCacheDBName
      BillingMode: PAY_PER_REQUEST
      AttributeDefinitions:
        - AttributeName: FindingId
          AttributeType: S
        - AttributeName: RuleId
          AttributeType: S
      KeySchema:
        - AttributeName: FindingId
          KeyType: HASH
        - AttributeName: RuleId
          KeyType: RANGE

  # ============================================================================
  # SSM Parameter for Supported Regions
  # ============================================================================
  SupportedRegionsParameter:
    Type: AWS::SSM::Parameter
    Properties:
      Name: !Sub '/security-tools/suppression-engine-${Environment}/supported-regions'
      Type: String
      Value: 'us-east-1,us-west-2'
      Description: Comma-separated list of supported regions

  # ============================================================================
  # IAM Role for Lambda
  # ============================================================================
  LambdaExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub 'inspector-validation-test-role-${Environment}'
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: DynamoDBAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - dynamodb:Scan
                  - dynamodb:Query
                  - dynamodb:GetItem
                  - dynamodb:PutItem
                  - dynamodb:DeleteItem
                  - dynamodb:BatchWriteItem
                Resource:
                  - !GetAtt SuppressionRulesTable.Arn
                  - !GetAtt CacheTable.Arn
        - PolicyName: SSMAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - ssm:GetParameter
                  - ssm:PutParameter
                Resource: !Sub 'arn:aws:ssm:${AWS::Region}:${AWS::AccountId}:parameter/security-tools/*'
        - PolicyName: SecurityHubAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - securityhub:GetFindings
                  - securityhub:BatchUpdateFindings
                Resource: '*'

  # ============================================================================
  # Lambda Function - Complete v2 Lambda with L1/L2 changes
  # ============================================================================
  SuppressionEngine:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub 'inspector-validation-test-${Environment}'
      Handler: index.lambda_handler
      Description: 'Complete v2 Suppression Engine Lambda with L1/L2 validation changes for testing'
      Runtime: python3.13
      Role: !GetAtt LambdaExecutionRole.Arn
      Timeout: 900
      Environment:
        Variables:
          eSecHubSuppressTableName: !Ref pSuppressionRuleDynamodbTableName
          eSecHubSuppressHashKey: 'id'
          eSecHubSuppressHashRange: 'ser_id'
          eSecHubSuppressCache: !Ref pSuppressionEngineCacheDBName
          EXECUTION_MODE: 'log'
          SUPPORTED_REGIONS_SSM_PARAMETER: !Sub '/security-tools/suppression-engine-${Environment}/supported-regions'
          LOGGING_LEVEL: 'INFO'
      Code:
        ZipFile: |
          import logging
          import os
          import re
          from datetime import datetime, timezone
          from typing import List, Dict, Optional
          from boto3.dynamodb.conditions import Key, Attr
          import boto3

          logger = logging.getLogger()
          RULES_TABLE_NAME = os.environ['eSecHubSuppressTableName']
          CACHE_TABLE_NAME = os.environ['eSecHubSuppressCache']
          EXECUTION_MODE = os.environ.get("EXECUTION_MODE", "log").lower()

          LOGGING_LEVEL = os.getenv("LOGGING_LEVEL", "INFO").upper()
          if LOGGING_LEVEL not in ["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"]:
            LOGGING_LEVEL = "INFO"
          logging.basicConfig(level=getattr(logging, LOGGING_LEVEL))
          logger = logging.getLogger(__name__)
          logger.setLevel(getattr(logging, LOGGING_LEVEL))

          dynamodb = boto3.resource('dynamodb')
          securityhub = boto3.client('securityhub')
          ssm = boto3.client("ssm")

          class SecurityHubFinding:

            def __init__(self, payload):
              self.product_name = (
                payload.get("ProductName") or
                payload.get("ProductFields", {}).get("aws/securityhub/ProductName") or
                "UnknownProduct"
              )
              self.finding_identifiers = []
              self.title = payload["Title"]
              self.label = payload["Severity"]["Label"]
              self.resource_id = ''
              self.finding_account_id = payload["AwsAccountId"]
              self.extra_resource_arn = ''
              self.id = ''
              self.generator_id = payload["GeneratorId"]
              self.resource_type = ''
              self.workflow_status = payload["Workflow"]["Status"]

              if self.product_name == "Security Hub" and 'Compliance' in payload and 'SecurityControlId' in payload["Compliance"]:
                self.id = payload["Compliance"]["SecurityControlId"]
              elif self.product_name == "Inspector" and 'Vulnerabilities' in payload and 'Id' in payload["Vulnerabilities"][0]:
                self.id = payload["Vulnerabilities"][0]["Id"]
              elif self.product_name in ("IAM Access Analyzer", "Health", "GuardDuty") and 'Resources' in payload and 'Id' in payload["Resources"]:
                self.id = payload["Resources"]["Id"]
              resource_one = payload["Resources"][0]
              finding_identifier = {
                "Id": payload["Id"],
                "ProductArn": payload["ProductArn"],
              }
              self.finding_identifiers.append(finding_identifier)
              self.finding_account_id = payload["AwsAccountId"]
              self.resource_id = resource_one["Id"]
              self.resource_type = resource_one["Type"]
              if "Details" in resource_one and "AwsEc2Instance" in resource_one["Details"] and "IamInstanceProfileArn" in resource_one["Details"]["AwsEc2Instance"]:
                self.extra_resource_arn = resource_one["Details"]["AwsEc2Instance"]["IamInstanceProfileArn"]
              elif "Details" in resource_one and "AwsEc2SecurityGroup" in resource_one["Details"] and "GroupName" in resource_one["Details"]["AwsEc2SecurityGroup"]:
                self.extra_resource_arn = resource_one["Details"]["AwsEc2SecurityGroup"]["GroupName"]
              elif "Details" in resource_one and "AwsEc2Instance" in resource_one["Details"] and "KeyName" in resource_one["Details"]["AwsEc2Instance"]:
                self.extra_resource_arn = resource_one["Details"]["AwsEc2Instance"]["KeyName"]
              self.note = payload.get("Note")
              self.udf = payload.get("UserDefinedFields", {})

            def __str__(self):
              return (
                  f"SecurityHubFinding("
                  f"id={self.id}, "
                  f"product_name={self.product_name}, "
                  f"title={self.title!r}, "
                  f"severity={self.label}, "
                  f"resource_id={self.resource_id}, "
                  f"resource_type={self.resource_type}, "
                  f"status={self.workflow_status}, "
                  f"finding_identifiers={self.finding_identifiers}, "
                  f"udf={self.udf}, "
                  f"notes={self.note})"
              )

            @staticmethod
            def from_api_finding(finding: dict) -> "SecurityHubFinding":
              return SecurityHubFinding(finding)

            def to_dict(self):
              return {
                  "id": self.id,
                  "product_name": self.product_name,
                  "title": self.title,
                  "label": self.label,
                  "resource_id": self.resource_id,
                  "finding_account_id": self.finding_account_id,
                  "extra_resource_arn": self.extra_resource_arn,
                  "generator_id": self.generator_id,
                  "resource_type": self.resource_type,
                  "workflow_status": self.workflow_status,
                  "finding_identifiers": self.finding_identifiers,
                  "notes": self.note,
                  "udf": self.udf
              }

          class Rule:
            def __init__(self, rule_data: Dict):
              self.id = rule_data['id']
              self.product_name = rule_data.get('product_name')
              self.ser_id = rule_data.get('ser_id')
              self.ser_link = rule_data.get('ser_link')
              self.account_exception = [a.strip() for a in rule_data.get('account_exception', '').split(',') if a.strip()]
              self.account_inclusion = [a.strip() for a in rule_data.get('account_inclusion', '').split(',') if a.strip()]
              self.resource_type = rule_data.get('resource_type', '')
              self.finding_title = rule_data.get('finding_title', '')
              self.resource_pattern = rule_data.get('resource_pattern', '')
              self.extra_resource_pattern = rule_data.get('extra_resource_pattern', '')
              self.from_severity = rule_data.get('from_severity', '')
              self.to_severity = rule_data.get('to_severity', '')
              self.due_date = self._parse_due_date(rule_data.get('due_date'))
              self.valid_rule = self._validate_rule()

            def _validate_rule(self):
              if self.product_name == "Inspector":
                # ResourcePattern + ID cross-validation
                trimmed_id = self.id.strip() if self.id else ""
                if len(self.resource_pattern) > 0:
                  # L2: Block wildcards in resource_pattern when ID is *
                  if trimmed_id == "*" and "*" in self.resource_pattern:
                    logger.error(f"Suppression rule (Id - {self.id}) has wildcard in resource_pattern when ID is * - {self.resource_pattern}")
                    return False
                  # L1: Allow wildcards in resource_pattern when ID is specific - PASS
                
                # L5: Block wildcards in resource_type ALWAYS (original behavior)
                if len(self.resource_type) > 0 and "*" in self.resource_type:
                  logger.error(f"Suppression rule (Id - {self.id}) has wildcard in resource_type - {self.resource_type}")
                  return False                
                if len(self.resource_pattern) == 0 and len(self.resource_type) == 0:
                  logger.error(f"Please provide both or one of ResourceType or ResourcePattern or Id")
                  return False
              elif len(self.resource_pattern) == 0 and len(self.resource_type) == 0:
                  return False
              return True

            def _parse_due_date(self, due_date: Optional[str]) -> Optional[datetime]:
              if not due_date:
                return None
              for fmt in ('%Y-%m-%d', '%m/%d/%Y', '%Y/%d/%m'):
                try:
                  return datetime.strptime(due_date, fmt).replace(tzinfo=timezone.utc)
                except ValueError:
                  continue
              logger.error(f"Failed to parse due date '{due_date}' with supported formats.")
              return None

            def is_expired(self) -> bool:
              return self.due_date and datetime.now(timezone.utc).date() > self.due_date.date()

            def matches(self, finding: SecurityHubFinding) -> Optional[Dict]:
              id_match = False
              if finding.id == self.id:
                id_match = True
                logger.debug(f"Suppression rule has matched ID: {finding.id} and Rule Id - {self.id}")
              else:
                if self.product_name == "Inspector":
                  id_prefix = re.compile(r'^' + self.id.strip("*"))
                  if id_prefix.search(finding.id):
                    id_match = True
                    logger.debug(f"Suppression rule has matched ID: {finding.id} and Vulnerability Id - {self.id}")
              is_resource_id_type_match = self._match_resource_id_type(finding)
              if id_match and \
                      is_resource_id_type_match and \
                      ((self.account_inclusion and finding.finding_account_id in self.account_inclusion) or \
                       (not self.account_inclusion and finding.finding_account_id not in self.account_exception)) and \
                      re.search(self.extra_resource_pattern, finding.extra_resource_arn):
                logger.debug(f"Finding event matched the rule id {self.id}")
                if self.from_severity and self.to_severity and finding.label == self.from_severity:
                  return {"action": "severity_update", "new_severity": self.to_severity}
                return {"action": "suppress", "new_severity": None}
              return None

            def _match_resource_id_type(self, finding: SecurityHubFinding):
              if len(self.resource_pattern) > 0 and len(self.resource_type) > 0:
                if re.search(self.resource_pattern, finding.resource_id) and re.search(self.resource_type,finding.resource_type):
                  logger.debug(f"Suppression rule matched - Resource Id and Resource Type")
                  return True
                else:
                  logger.debug(f"Suppression rule not matched - DB Resource Id: {self.resource_pattern} and DB Resource Type: {self.resource_type}")
                  return False
              elif len(self.resource_pattern) > 0 and re.search(self.resource_pattern, finding.resource_id):
                logger.debug(f"Suppression rule  matched - Resource Id")
                return True
              elif len(self.resource_type) > 0 and re.search(self.resource_type, finding.resource_type):
                logger.debug(f"Suppression rule matched - Resource Type")
                return True
              else:
                logger.debug(f"Suppression rule not matched - Either Resource Id or  Resource Type is empty")
                return False

          class SuppressionRuleEngine:
            def __init__(self):
              self.rules_table = dynamodb.Table(RULES_TABLE_NAME)
              self.cache_table = dynamodb.Table(CACHE_TABLE_NAME)
              self.rules: List[Rule] = self.load_rules()
              self.rule_ids = {rule.id for rule in self.rules}

              self.supported_regions_ssm_param_name = os.environ.get("SUPPORTED_REGIONS_SSM_PARAMETER","/security-tools/suppression-engine/supported-regions")
              self.supported_regions = self._get_supported_regions()
              if not self.supported_regions:
                  raise ValueError("SUPPORTED_REGIONS var is empty")

            def _get_supported_regions(self) -> list[str]:
              try:
                response = ssm.get_parameter(Name=self.supported_regions_ssm_param_name)
                regions = response["Parameter"]["Value"].split(",")
                return [r.strip() for r in regions if r.strip()]
              except Exception as e:
                logger.error(f"Failed to read ssm param. Error : {e}")

            def rotate_supported_regions(self):
              if self.supported_regions:
                first = self.supported_regions.pop(0)
                self.supported_regions.append(first)
                ssm.put_parameter(
                    Name=self.supported_regions_ssm_param_name,
                    Value=",".join(self.supported_regions),
                    Type="String",
                    Overwrite=True
                )
                logger.debug(f"Rotated regions with new order as : {self.supported_regions}")

            def _batch_update_findings(self, **kwargs):
              if EXECUTION_MODE == "execute":
                return securityhub.batch_update_findings(**kwargs)
              else:
                logger.info(f"[LOG MODE] Would call batch_update_findings with: {kwargs}")
                return {"ProcessedFindings": [], "UnprocessedFindings": []}

            def load_rules(self) -> List[Rule]:
              response = self.rules_table.scan()
              rules = [Rule(item) for item in response['Items']]
              logger.info(f"Loaded {len(rules)} suppression rules.")
              return rules

            def cache_eligible_for_suppression_finding(
                        self,
                        finding: SecurityHubFinding,
                        event_type: str = "current",
                        old_status: str = "NEW",
                        new_status: str = "SUPPRESSED"):
              logger.info(f"Checking if eligible for {event_type} event with finding as {finding}")
              finding_id_arn = finding.finding_identifiers[0]['Id']
              for rule in self.rules:
                if rule.is_expired():
                  logger.info(f"rule {rule.id} is expired so skipping match.")
                  continue
                if not rule.valid_rule:
                  logger.info(f"rule {rule.id} is not valid so skipping match.")
                  continue
                match_result = rule.matches(finding)
                if match_result:
                  item = {
                      'FindingId': finding_id_arn,
                      'RuleId': rule.id,
                      'SerId': rule.ser_id,
                      'SerLink': rule.ser_link,
                      'Finding': finding.to_dict(),
                      'Action': match_result["action"],
                      'OldStatus': old_status,
                      'NewStatus': new_status
                  }
                  if match_result["action"] == "severity_update":
                    item["NewSeverity"] = match_result["new_severity"]
                  if rule.due_date:
                    item["SerDueDate"] = rule.due_date.strftime('%m/%d/%Y')
                  if event_type == "historic" and old_status == "SUPPRESSED" and finding.udf and 'RuleId' in finding.udf:
                    if finding.udf['RuleId'] != rule.id:
                      self.cache_table.put_item(Item=item)
                      logger.info(f"Caching historic finding {finding_id_arn} matched with rule {rule.id} which is eligible for re-suppression.")
                      return
                    else:
                      logger.info(f"Historic finding {finding_id_arn} matched with rule {rule.id} but was already suppressed with this rule so skipping.")
                      return
                  else:
                    self.cache_table.put_item(Item=item)
                    logger.info(f"Caching finding {finding_id_arn} matched with rule {rule.id} which is eligible for suppression.")
                    return
                else:
                  logger.info(f"No match found for rule {rule.id} for finding : {finding}.")

              if event_type == "historic" and old_status == "SUPPRESSED":
                if finding.udf and 'RuleId' in finding.udf:
                  item = {
                      'FindingId': finding_id_arn,
                      'RuleId': finding.udf['RuleId'],
                      'SerId': finding.udf['SerId'],
                      'SerLink': finding.udf['SerLink'],
                      'Finding': finding.to_dict(),
                      'Action': "unsuppress",
                      'OldStatus': old_status,
                      'NewStatus': new_status
                  }
                  self.cache_table.put_item(Item=item)
                  logger.info(f"Historic finding {finding_id_arn} and now has no rule matched so eligible for un-suppress.")
              else:
                logger.info(f"No rule matched for {finding_id_arn} so it is not eligible for suppression.")

            def process_findings_from_cache_for_deleted_rules(self):
              response = self.cache_table.scan()
              items = response.get('Items', [])
              if not items:
                return
              deleted_rule_items = [item for item in items if item.get("RuleId") not in self.rule_ids]
              if not deleted_rule_items:
                return
              logger.info(f"Found {len(deleted_rule_items)} cached findings tied to deleted rules.")
              for i in range(0, len(deleted_rule_items), 100):
                chunk = deleted_rule_items[i:i + 100]
                identifiers = []
                ident_to_cachekeys = {}
                for f in chunk:
                  for ident in f["Finding"]["finding_identifiers"]:
                    tup = (ident["Id"], ident["ProductArn"])
                    identifiers.append({"Id": tup[0], "ProductArn": tup[1]})
                    ident_to_cachekeys[tup] = {"FindingId": f["FindingId"], "RuleId": f["RuleId"]}

                if not identifiers:
                  continue

                deleted_rule_id = chunk[0].get("RuleId", "unknown-rule")
                now_str = datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%SZ")
                new_note_text = f"[{now_str}] RuleId={deleted_rule_id} deleted | Action=unsuppress"

                first_item = chunk[0]
                existing_note = first_item["Finding"].get("notes", {})
                if existing_note and isinstance(existing_note, dict) and "Text" in existing_note:
                  combined_text = f"{existing_note['Text']}\n---\n{new_note_text}"
                else:
                  combined_text = new_note_text

                existing_udf = first_item["Finding"].get("udf", {}) or {}
                udf = dict(existing_udf)
                udf["Reason"] = "deleted"
                udf["DeletedRuleId"] = deleted_rule_id

                update_kwargs = {
                        "FindingIdentifiers": identifiers,
                        "Workflow": {"Status": "NEW"},
                        "Note": {
                            "Text": combined_text,
                            "UpdatedBy": "SuppressionRuleEngine"
                        },
                        "UserDefinedFields": {"RuleId": "deleted"}
                    }

                logger.info(f"Unsuppressing {len(identifiers)} findings due to deleted RuleId={deleted_rule_id}...")

                try:
                  resp = self._batch_update_findings(**update_kwargs)
                  processed_entries = resp.get("ProcessedFindings", [])
                  processed_ids = set()
                  for p in processed_entries:
                    if "FindingIdentifier" in p:
                      fid = p["FindingIdentifier"]
                      processed_ids.add((fid.get("Id"), fid.get("ProductArn")))
                    else:
                      processed_ids.add((p.get("Id"), p.get("ProductArn")))

                  if processed_ids:
                    with self.cache_table.batch_writer() as writer:
                      for ident_tuple in processed_ids:
                        cache_keys = ident_to_cachekeys.get(ident_tuple)
                        if cache_keys:
                          try:
                            writer.delete_item(Key={"FindingId": cache_keys["FindingId"], "RuleId": cache_keys["RuleId"]})
                            logger.info(f"Deleted cache item for FindingId={cache_keys['FindingId']} RuleId={cache_keys['RuleId']}")
                          except Exception as e:
                            logger.error(f"Failed deleting cache item {cache_keys}: {e}")

                except Exception as e:
                  logger.error(f"batch_update_findings failed while unsuppressing deleted-rule findings: {e}")
                  continue

            def process_findings_from_cache(self):
              for rule in self.rules:
                response = self.cache_table.scan(FilterExpression=Attr("RuleId").eq(rule.id),)
                items = response.get("Items", [])
                if not items:
                  logger.debug(f'No findings exists in cache for rule id : {rule.id} so moving on...')
                  continue
                logger.debug(f"Processing {len(items)} cached findings for RuleId={rule.id}")
                for i in range(0, len(items), 100):
                  chunk = items[i:i + 100]
                  identifiers = []
                  ident_to_cachekeys = {}
                  for f in chunk:
                    for ident in f["Finding"]["finding_identifiers"]:
                      tup = (ident["Id"], ident["ProductArn"])
                      identifiers.append({"Id": tup[0], "ProductArn": tup[1]})
                      ident_to_cachekeys[tup] = {"FindingId": f["FindingId"], "RuleId": f["RuleId"]}

                  if not identifiers:
                    logger.info(f"No identifiers found in chunk for RuleId={rule.id}, skipping chunk.")
                    continue

                  first_item = chunk[0]
                  action = first_item["Action"]
                  new_severity = first_item.get("NewSeverity")
                  ser_id = first_item.get("SerId")
                  ser_link = first_item.get("SerLink")
                  due_date = first_item.get("SerDueDate")

                  update_kwargs = {"FindingIdentifiers": identifiers}

                  if action == "suppress":
                    update_kwargs["Workflow"] = {"Status": "SUPPRESSED"}
                  elif action == "unsuppress":
                    update_kwargs["Workflow"] = {"Status": "NEW"}
                  elif action == "severity_update" and new_severity:
                    update_kwargs["Severity"] = {"Label": new_severity}

                  notes = [ser_link] if ser_link else []
                  if due_date:
                    notes.append(f"Due Date: {due_date}")
                  current_dt = datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%SZ")
                  new_note_text = f"[{current_dt}] Rule {rule.id} | Action={action} | {' | '.join(notes)}".strip(" |")

                  existing_note = first_item["Finding"].get("notes", {})
                  if existing_note and isinstance(existing_note, dict) and "Text" in existing_note:
                    combined_text = f"{existing_note['Text']}\n---\n{new_note_text}"
                  else:
                    combined_text = new_note_text

                  update_kwargs["Note"] = {
                      "Text": combined_text,
                      "UpdatedBy": "SuppressionRuleEngine"
                  }

                  udf = {"RuleId": rule.id, "SerId": ser_id}
                  if ser_link:
                    udf["SerLink"] = ser_link
                  if due_date:
                    udf["SerDueDate"] = due_date
                  update_kwargs["UserDefinedFields"] = udf

                  ids_to_log = [f["Id"] for f in identifiers]
                  logger.info(f"[Rule {rule.id}] Updating {len(ids_to_log)} findings: {ids_to_log}")
                  try:
                    resp = self._batch_update_findings(**update_kwargs)
                    processed_entries = resp.get("ProcessedFindings", [])
                    processed_ids = set()

                    for p in processed_entries:
                      if "FindingIdentifier" in p and isinstance(p["FindingIdentifier"], dict):
                        fid = p["FindingIdentifier"]
                        processed_ids.add((fid.get("Id"), fid.get("ProductArn")))
                      else:
                        processed_ids.add((p.get("Id"), p.get("ProductArn")))

                    if processed_ids:
                      with self.cache_table.batch_writer() as writer:
                        for ident_tuple in processed_ids:
                          cache_keys = ident_to_cachekeys.get(ident_tuple)
                          if cache_keys:
                            try:
                              writer.delete_item(Key={"FindingId": cache_keys["FindingId"], "RuleId": cache_keys["RuleId"]})
                              logger.debug(f"Deleted cache item for FindingId={cache_keys['FindingId']} RuleId={cache_keys['RuleId']}")
                            except Exception as e:
                              logger.error(f"Failed deleting cache item for {cache_keys}: {e}")
                    else:
                      logger.info(f"No processed findings returned for RuleId={rule.id} in this chunk.")

                  except Exception as e:
                    logger.error(f"batch_update_findings failed for RuleId={rule.id}: {e}")
                    continue

            def reevaluate_historic_suppressed_findings(self):
              logger.info(f"Will work on suppressed historic findings in {self.supported_regions[0]}")
              paginator = securityhub.get_paginator("get_findings")
              try:
                for page_num, page in enumerate(
                        paginator.paginate(
                            Filters={
                                "WorkflowStatus": [{"Value": "SUPPRESSED", "Comparison": "EQUALS"}],
                                "Region": [{"Value": self.supported_regions[0], "Comparison": "EQUALS"}]
                            }
                        ),
                        start=1
                ):
                  findings = page.get("Findings", [])
                  logger.debug(f"Suppressed findings page {page_num}: {len(findings)} findings returned")
                  for finding in findings:
                    sec_hub_finding = SecurityHubFinding(finding)
                    udf = sec_hub_finding.udf
                    rule_id = udf.get("RuleId") if udf else None
                    if not rule_id:
                      logger.info(f"Suppressed finding {sec_hub_finding.id} has no RuleId in UDF so reevaluating")
                    elif rule_id not in self.rule_ids:
                      logger.info(f"Suppressed finding {sec_hub_finding.id} has invalid RuleId={rule_id} so reevaluating")
                    else:
                      rule_obj = next((r for r in self.rules if r.id == rule_id), None)
                      if not rule_obj.is_expired():
                        logger.info(f"Suppressed finding {sec_hub_finding.id} already has current RuleId={rule_id} so skipping")
                        continue
                      else:
                        logger.info(f"Suppressed finding {sec_hub_finding.id} already has current RuleId={rule_id} but now that rule is either expired or invalid so will reevaluate")
                    self.cache_eligible_for_suppression_finding(
                        finding=sec_hub_finding,
                        event_type="historic",
                        old_status="SUPPRESSED",
                        new_status="NEW"
                    )
              except Exception as e:
                logger.error(f'Exception happened while getting finding for SUPPRESSED so stopping further processing. Error : {e}')
                return

            def reevaluate_historic_new_findings(self, context):
              logger.debug(f"Will work on new historic findings in {self.supported_regions[0]}")
              paginator = securityhub.get_paginator("get_findings")
              try:
                for page_num, page in enumerate(
                        paginator.paginate(
                            Filters={
                                "WorkflowStatus": [{"Value": "NEW", "Comparison": "EQUALS"}],
                                "Region": [{"Value": self.supported_regions[0], "Comparison": "EQUALS"}]
                            }
                        ),
                        start=1
                ):
                  remaining_ms = context.get_remaining_time_in_millis()
                  if remaining_ms < 3 * 60 * 1000:
                    logger.debug(f'Lambda timeout will be in {remaining_ms / 1000}s so returning back from here...')
                    return
                  findings = page.get("Findings", [])
                  logger.debug(f"NEW findings page {page_num}: {len(findings)} findings returned")
                  for finding in findings:
                    sec_hub_finding = SecurityHubFinding(finding)
                    self.cache_eligible_for_suppression_finding(
                        finding=sec_hub_finding,
                        event_type="historic",
                        old_status="NEW",
                        new_status="SUPPRESSED"
                    )
              except Exception as e:
                logger.error(f'Exception happened while getting finding for NEW so stopping further processing. Error : {e}')
                return

          def lambda_handler(event, context):
            logger.info(f"Incoming event : {repr(event)}")
            suppression_engine = SuppressionRuleEngine()
            scheduler_type = event.get("scheduler-type")

            # --- Case 2: New Security Hub finding ---
            if event.get("source") == "aws.securityhub":
              logger.info("Event type : SecurityHub")
              findings = event.get("detail", {}).get("findings", [])
              for finding in findings:
                sec_hub_finding = SecurityHubFinding(finding)
                logger.info(f"Transformed event to SecurityHubFinding as {sec_hub_finding}")
                suppression_engine.cache_eligible_for_suppression_finding(
                    finding=sec_hub_finding,
                    event_type="SecurityHub"
                )

            # --- Case 3: Scheduled rule ---
            elif scheduler_type:
              if scheduler_type == 'historic-suppressed':
                logger.info("Event type : Historic Re-evaluate suppressed findings")
                suppression_engine.reevaluate_historic_suppressed_findings()
              elif scheduler_type == 'historic-new':
                logger.info("Event type : Historic Re-evaluate new findings")
                suppression_engine.reevaluate_historic_new_findings(context)
                suppression_engine.rotate_supported_regions()
              elif scheduler_type == 'current':
                logger.info("Event type : Suppress existing findings")
                suppression_engine.process_findings_from_cache()
                suppression_engine.process_findings_from_cache_for_deleted_rules()
            else:
              logger.warning("Unknown event type received")

Outputs:
  RulesTableName:
    Value: !Ref SuppressionRulesTable
    Export:
      Name: !Sub '${Environment}-RulesTableName'
  
  CacheTableName:
    Value: !Ref CacheTable
    Export:
      Name: !Sub '${Environment}-CacheTableName'
  
  LambdaFunctionName:
    Value: !Ref SuppressionEngine
    Export:
      Name: !Sub '${Environment}-LambdaFunctionName'
  
  LambdaFunctionArn:
    Value: !GetAtt SuppressionEngine.Arn
    Export:
      Name: !Sub '${Environment}-LambdaFunctionArn'
