AWSTemplateFormatVersion: "2010-09-09"
Description: SecurityHub findings suppression

Parameters:
  pTagKey1:
    Type: String
    Description: Tag key

  pTagValue1:
    Type: String
    Description: Tag key value

  pTagApplicationKey:
    Description: Application TAG key
    Type: String

  pTagApplicationValue:
    Description: Application TAG value
    Type: String

  pTagOwnerKey:
    Description: Owner TAG key
    Type: String

  pTagOwnerValue:
    Description: Owner TAG value
    Type: String

  pTagTechnicalContactKey:
    Description: Technical Contact TAG key
    Type: String

  pTagTechnicalContactValue:
    Description: Technical Contact TAG value
    Type: String

  pTagProductKey:
    Description: Product TAG key
    Type: String

  pTagProductValue:
    Description: Product TAG value
    Type: String

  pTagEnvKey:
    Description: Env TAG key
    Type: String

  pTagEnvValue:
    Description: Env TAG value
    Type: String

  pTagCreatedByKey:
    Description: CreatedBy TAG key
    Type: String

  pTagCreatedByValue:
    Description: CreatedBy TAG value
    Type: String

  pTagCostCenterIdKey:
    Description: CostCenterId TAG key
    Type: String

  pTagCostCenterIdValue:
    Description: CostCenterId TAG value
    Type: String

  pSuppressionRuleDynamodbTableName:
    Type: String
    Description: The name of the Dynamodb table to store the Suppression rules.
    Default: "sw-securityhub-suppression-ser-db-v2-main-aws"

  pSuppressionEngineCacheDBName:
    Type: String
    Description: The name of the Dynamodb table used to cache the matching findings.
    Default: "sw-securityhub-suppression-cache-main-aws"

  pSuppressionRuleDynamoDbStreamArn:
    Type: String
    Description: Suppression Rule DynamoDb Stream Arn
  pLambdaName:
    Type: String
    Description: Lambda Function Name
    Default: sw-security-hub-batch-update-suppression-lambda-main-aws

  pSuppressionEngineLambdaName:
    Type: String
    Description: Suppression engine lambda function name
    Default: sw-security-hub-batch-update-suppression-lambda-v2-main-aws

  pLambdaExecutionRoleName:
    Type: String
    Description: Lambda Role Name
    Default: sw-security-hub-batch-update-suppression-role-main-aws
  pResetLambdaName:
    Type: String
    Description: The name of the Lambda function to reset the Workflow status of SecurityHub findings when an item is created/updated/deleted in the CSCI DynamoDB table.
    Default: sw-security-hub-batch-update-reset-suppression-lambda
  pResetLambdaExecutionRoleName:
    Type: String
    Description: The name of the Lambda Role to reset the workflow status from Suppression to New
    Default: sw-security-hub-batch-update-reset-suppression-role

  pSupportedRegionsSsmParameterName:
    Type: String
    Description: SSM Parameter containing list of supported regions

  pSupportedRegions:
    Type: String
    Description: Comma separated list of supported regions




Resources:
  rLambdaExecutionRole:
    Type: "AWS::IAM::Role"
    Properties:
      RoleName: !Ref pLambdaExecutionRoleName
      AssumeRolePolicyDocument:
        Statement:
          - Action: "sts:AssumeRole"
            Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
        Version: 2012-10-17
      Description: Suppression role for Security Hub batch update findings
      ManagedPolicyArns:
        - !Ref rLambdaExecutionPolicy
      Path: /

  rSecHubFunction:
    Type: "AWS::Lambda::Function"
    Properties:
      Code:
        ZipFile: |
          from __future__ import print_function
          import json
          import logging
          import boto3
          import re
          import os
          import sys
          import traceback
          from datetime import date
          import botocore.exceptions as boto3exceptions

          logger = logging.getLogger()
          logger.setLevel(logging.INFO)

          client = boto3.client("securityhub")
          dynamodb = boto3.client('dynamodb')

          sechub_table_name = os.environ['eSecHubSuppressTableName']
          sechub_hash_key = os.environ['eSecHubSuppressHashKey']
          sechub_hash_range = os.environ['eSecHubSuppressHashRange']


          class RecordsHandler:
              def __init__(self):
                  self.finding_identifiers = []
                  self.title = ''
                  self.label = ''
                  self.resource_id = ''
                  self.finding_account_id = ''
                  self.product_name = ''
                  self.extra_resource_arn = ''
                  self.id = ''
                  self.generator_id = ''
                  self.resource_type = ''
                  self.workflow_status = ''

              @property
              def get_id(self):
                  return self.id

              @property
              def get_finding_identifiers(self):
                  return self.finding_identifiers

              @property
              def get_finding_title(self):
                  return self.title

              @property
              def get_finding_severity(self):
                  return self.label

              @property
              def get_workflow_status(self):
                  return self.workflow_status

              @property
              def get_product_name(self):
                  return self.product_name

              @property
              def get_finding_account_id(self):
                  return self.finding_account_id

              @property
              def get_finding_resource_type(self):
                  return self.resource_type

              @property
              def get_finding_resource_id(self):
                  return self.resource_id

              @property
              def get_finding_extra_resource_arn(self):
                  return self.extra_resource_arn


              def record_handler(self, payload):
                  # logger.info(f"payload {payload}")

                  try:

                      self.product_name = payload["detail"]["findings"][0]["ProductName"]

                      # Based on the Finding Type - Standards, Vulnerability and Config get the respective id
                      #   This a is unique Id to identify the affected resource and rule#. Some examples:
                      #   SecurityHub has attributes: Compliance, SecurityControlId and "ProductName": "Security Hub"
                      #   Inspector has attributes: Vulnerabilities, Id and "ProductName": "Inspector",
                      #   Config has attributes: Resources, Id and "ProductName": "Config"
                      #   Access Analyzer has attributes: Resources, Id and "ProductName": "IAM Access Analyzer"
                      #   Health has attributes: Resources, Id and "ProductName": "Health",
                      #   Guard Duty has attributes: Resources, Id and "ProductName": "GuardDuty"

                      if self.product_name == "Security Hub" and 'Compliance' in payload["detail"]["findings"][0] \
                              and 'SecurityControlId' in payload["detail"]["findings"][0]["Compliance"]:

                          self.id = payload["detail"]["findings"][0]["Compliance"]["SecurityControlId"]
                      elif self.product_name == "Inspector" and 'Vulnerabilities' in payload["detail"]["findings"][0] and 'Id' in \
                              payload["detail"]["findings"][0]["Vulnerabilities"][0]:

                          self.id = payload["detail"]["findings"][0]["Vulnerabilities"][0]["Id"]
                      elif self.product_name in ("IAM Access Analyzer", "Health", "GuardDuty") and 'Resources' in \
                              payload["detail"]["findings"][0] and 'Id' in payload["detail"]["findings"][0]["Resources"]:

                          self.id = payload["detail"]["findings"][0]["Resources"]["Id"]

                      resource_one = payload["detail"]["findings"][0]["Resources"][0]
                      finding_identifier = {
                          "Id": payload["detail"]["findings"][0]["Id"],
                          "ProductArn": payload["detail"]["findings"][0]["ProductArn"],
                      }

                      self.generator_id = payload["detail"]["findings"][0]["GeneratorId"]
                      self.finding_identifiers.append(finding_identifier)
                      self.title = payload["detail"]["findings"][0]["Title"]
                      self.label = payload["detail"]["findings"][0]["Severity"]["Label"]
                      self.workflow_status = payload["detail"]["findings"][0]["Workflow"]["Status"]
                      self.finding_account_id = payload["detail"]["findings"][0]["AwsAccountId"]
                      self.resource_id = resource_one["Id"]
                      self.resource_type = resource_one["Type"]

                      if "Details" in resource_one and "AwsEc2Instance" in resource_one["Details"] and "IamInstanceProfileArn" in \
                              resource_one["Details"]["AwsEc2Instance"]:

                          self.extra_resource_arn = resource_one["Details"]["AwsEc2Instance"]["IamInstanceProfileArn"]

                      elif "Details" in resource_one and "AwsEc2SecurityGroup" in resource_one["Details"] and "GroupName" in \
                              resource_one["Details"]["AwsEc2SecurityGroup"]:

                          self.extra_resource_arn = resource_one["Details"]["AwsEc2SecurityGroup"]["GroupName"]

                      elif "Details" in resource_one and "AwsEc2Instance" in resource_one["Details"] and "KeyName" in \
                              resource_one["Details"]["AwsEc2Instance"]:

                          self.extra_resource_arn = resource_one["Details"]["AwsEc2Instance"]["KeyName"]

                  except Exception as exp:
                      exception_type, exception_value, exception_traceback = sys.exc_info()
                      traceback_string = traceback.format_exception(
                          exception_type, exception_value, exception_traceback)
                      err_msg = json.dumps({
                          "errorType": exception_type.__name__,
                          "errorMessage": str(exception_value),
                          "stackTrace": traceback_string
                      })
                      logger.error(err_msg)
                      logger.error(exp)
                      return logger.error(exp)



          def get_record_by_scan(product_name: str, finding_type: str, id_wildcard: str):

              row_count = 0
              try:
                  db_records = dynamodb.scan(
                      TableName=sechub_table_name,
                      FilterExpression="product_name = :product_name AND contains(id, :id_wildcard) AND finding_type = :finding_type",
                      ExpressionAttributeValues={
                          ':product_name': {'S': product_name},
                          ':finding_type': {'S': finding_type},
                          ':id_wildcard': {'S': id_wildcard}
                      }
                  )

                  if 'Count' in db_records:
                      row_count = int(db_records.get("Count", []))
                      logger.info(f"Total number of wild card suppression records: {row_count}")

              except Exception as exp:
                      exception_type, exception_value, exception_traceback = sys.exc_info()
                      traceback_string = traceback.format_exception(
                          exception_type, exception_value, exception_traceback)
                      err_msg = json.dumps({
                          "errorType": exception_type.__name__,
                          "errorMessage": str(exception_value),
                          "stackTrace": traceback_string
                      })
                      logger.error(err_msg)
                      logger.error(exp)
                      return row_count, logger.error(exp)

              else:
                  return row_count, db_records


          def get_record_by_query(id: str, product_name: str):

              row_count = 0
              try:
                  db_records = dynamodb.query(
                      TableName=sechub_table_name,
                      KeyConditionExpression=f'{sechub_hash_key} = :id',
                      FilterExpression="product_name = :product_name",
                      ExpressionAttributeValues={
                          ':id': {'S': id},
                          ':product_name': {'S': product_name}
                      }
                  )

                  if 'Count' in db_records:
                      row_count = int(db_records.get("Count", []))
                      logger.info(f"Total number of suppresion rule(s) matched the SecurityControlId/Vulnerability ID {id}: {row_count}")

              except Exception as exp:
                      exception_type, exception_value, exception_traceback = sys.exc_info()
                      traceback_string = traceback.format_exception(
                          exception_type, exception_value, exception_traceback)
                      err_msg = json.dumps({
                          "errorType": exception_type.__name__,
                          "errorMessage": str(exception_value),
                          "stackTrace": traceback_string
                      })
                      logger.error(err_msg)
                      logger.error(exp)
                      return row_count, logger.error(exp)
              else:
                  return row_count, db_records


          def match_resource_id_type (resource_type, resource_id, product_name, finding_resource_id, finding_resource_type):

              resourceValidation = False

              if len(resource_id) > 0 and len(resource_type) > 0:
                  if re.search(resource_id, finding_resource_id) and re.search(resource_type, finding_resource_type):
                      resourceValidation = True
                      logger.info(f"Suppression rule matched - Resource Id and Resource Type")
                  else:
                      logger.info(f"Suppression rule not matched - DB Resource Id: {resource_id} and DB Resource Type: {resource_type}")
              elif len(resource_id) > 0 and re.search(resource_id, finding_resource_id):
                  resourceValidation = True
                  logger.info(f"Suppression rule  matched - Resource Id")
              elif len(resource_type) > 0 and re.search(resource_type, finding_resource_type):
                  resourceValidation = True
                  logger.info(f"Suppression rule matched - Resource Type")
              elif len(resource_id) == 0 and len(resource_type) == 0 and not product_name == "Inspector" :
                  resourceValidation = True
                  logger.info(f"Suppression rule (Product is not Inspector) - Resource Id and Resource Type is empty")
              else:
                  logger.info(f"Suppression rule not matched - Either Resource Id or  Resource Type is empty")

              return resourceValidation


          def batch_update_findings(findings_record, db_records):
              row_count = 0
              id_match = True
              is_resource_id_type_match = False

              finding_identifiers = findings_record.get_finding_identifiers

              if 'Count' in db_records:
                  row_count = int(db_records.get("Count", []))
                  db_item = db_records.get("Items", [])

                  for i in range(0, row_count):
                      if 'id' in db_item[i]:
                          id = db_item[i]['id']['S']
                      else:
                          id = ""
                      if 'product_name' in db_item[i]:
                          product_name = db_item[i]['product_name']['S']
                      else:
                          product_name = ""
                      if 'ser_id' in db_item[i]:
                          ser_id = db_item[i]['ser_id']['S']
                      else:
                          ser_id = ""
                      if 'ser_link' in db_item[i]:
                          ser_link = db_item[i]['ser_link']['S']
                      else:
                          ser_link = ""
                      if 'account_exception' in db_item[i]:
                          account_exception = db_item[i]['account_exception']['S'].split(",")
                      else:
                          account_exception = ['']
                      if 'account_inclusion' in db_item[i]:
                          account_inclusion = db_item[i]['account_inclusion']['S'].split(",")
                      else:
                          account_inclusion = ['']
                      if 'resource_type' in db_item[i]:
                          resource_type = db_item[i]['resource_type']['S']
                      else:
                          resource_type = ""
                      if 'resource_pattern' in db_item[i]:
                          resource_pattern = db_item[i]['resource_pattern']['S']
                      else:
                          resource_pattern = ""
                      if 'extra_resource_pattern' in db_item[i]:
                          extra_resource_pattern = db_item[i]['extra_resource_pattern']['S']
                      else:
                          extra_resource_pattern = ""
                      if 'from_severity' in db_item[i]:
                          from_severity = db_item[i]['from_severity']['S']
                      else:
                          from_severity = ""
                      if 'to_severity' in db_item[i]:
                          to_severity = db_item[i]['to_severity']['S']
                      else:
                          to_severity = ""
                      if 'due_date' in db_item[i]:
                          due_date = db_item[i]['due_date']['S']
                      else:
                          due_date = ""

                      today = date.today().strftime("%Y-%m-%d")

                      logger.info(f"Comparing the suppression rule attributes for - id: {id} and Ser Id: {ser_id}")
                      # If the product name is "Inspector", ResourcePattern or ResourceType must not have wild cards and either one must have value
                      if product_name == "Inspector":
                          if len(resource_pattern) > 0 and re.search(resource_pattern, "*"):
                              logger.error(
                                  f"Suppression rule (Id - {id}) has wild card(*) in resource_pattern - {resource_pattern}")
                              raise ValueError(f"Suppression (Id - {id}) rule has wild card(*) in resource_pattern - {resource_pattern}")

                          if len(resource_type) > 0 and re.search(resource_type, "*"):
                              logger.error(f"Suppression rule (Id - {id}) has wild card(*) resource_type - {resource_type}")
                              raise ValueError(f"Suppression rule has wild card(*) resource_type - {resource_type}")

                          # Check for Resource Id match
                          if len(resource_pattern) == 0 and len(resource_type) == 0:
                              logger.error(f"Please provide both or one of ResourceType or ResourcePattern or Id")
                              raise ValueError(f"Please provide both or one of ResourceType or ResourcePattern or Id")

                          # Id (CVE Id) can be partial - *, CVE-*, CVE-12344-
                          # logger.info(f"******* Compare ID: {findings_record.id} and Vulnerability Id - {id}")
                          id_prefix = re.compile(r'^' + id.strip("*"))
                          if not (id_prefix.search(findings_record.id)):
                              id_match = False
                          else:
                              logger.info(f"Suppression rule has matched ID: {findings_record.id} and Vulnerability Id - {id}")

                      is_resource_id_type_match = match_resource_id_type(resource_type, resource_pattern, product_name, findings_record.get_finding_resource_id, findings_record.get_finding_resource_type)

                      if id_match and \
                         is_resource_id_type_match and \
                         ((account_inclusion != [''] and findings_record.get_finding_account_id in account_inclusion) or \
                         (account_inclusion == [''] and findings_record.get_finding_account_id not in account_exception)) and \
                         re.search(extra_resource_pattern, findings_record.get_finding_extra_resource_arn) and \
                         (not due_date or today <= due_date):

                          logger.info(f"Finding event matched the rule: Starting Batchupdate")
                          try:
                              severity = findings_record.get_finding_severity
                              workflow_status = "SUPPRESSED"
                              response = {}

                              if len(from_severity) > 0 and len(
                                      to_severity) > 0 and from_severity == findings_record.get_finding_severity:
                                  severity = to_severity
                                  response = client.batch_update_findings(
                                      FindingIdentifiers=finding_identifiers,
                                      Severity={"Label": severity},
                                      Note={"Text": "SER Link - " + ser_link,
                                          "UpdatedBy": "Landing Zone SER severity update or suppression add-on"},
                                      UserDefinedFields={"SerLink": ser_link, "SerDueDate": due_date}
                                  )

                              else:
                                  response = client.batch_update_findings(
                                      FindingIdentifiers=finding_identifiers,
                                      Severity={"Label": severity},
                                      Workflow={"Status": workflow_status},
                                      Note={"Text": "SER Link - " + ser_link,
                                          "UpdatedBy": "Landing Zone SER severity update or suppression add-on"},
                                      UserDefinedFields={"SerLink": ser_link, "SerDueDate": due_date}
                                  )

                              for processed_findings in response["ProcessedFindings"]:
                                  logger.info(
                                      f"processed and suppressed id {processed_findings['Id']} \
                                      productarn {processed_findings['ProductArn']}"
                                  )

                              for unprocessed_findings in response["UnprocessedFindings"]:
                                  logger.error(
                                      f"unprocessed finding id {unprocessed_findings['FindingIdentifier']['Id']} \
                                      productarn {unprocessed_findings['FindingIdentifier']['ProductArn']} error code \
                                      {unprocessed_findings['ErrorCode']} error message {unprocessed_findings['ErrorMessage']}"
                                  )
                          except boto3exceptions.ClientError as error:
                              logger.exception("client error")
                              raise ConnectionError(f"Client error invoking batch update findings {error}")
                          except boto3exceptions.ParamValidationError as error:
                              logger.exception("invalid parameters")
                              raise ValueError(f"The parameters you provided are incorrect: {error}")
                      else:
                          logger.info(f"Suppression rule did not match - id: {id} and Ser Id: {ser_id}")


          def handler(event, context):
              logger.info(f"Incoming event : {repr(event)}")
              rh = RecordsHandler()
              rh.record_handler(event)
              id = rh.get_id
              product_name = rh.get_product_name

              logger.info(f"finding_identifiers {rh.get_finding_identifiers}")

              if len(id) > 0 and len(product_name) > 0:

                  logger.info("Query DB: Suppression rules")
                  record_count, query_records = get_record_by_query(id, product_name)
                  if record_count > 0:
                      batch_update_findings(rh, query_records)

                  else:
                      logger.info("DB: Id/SecurityControlId match - No matching suppression rule")

                  logger.info("Scan DB: Wildcard rules for Inspector")
                  if product_name == "Inspector":
                      # Id can be * or CVE-* or CVE-3211-*
                      record_count, scan_records = get_record_by_scan(product_name, "Vulnerabilities", "*")
                      if record_count > 0:
                          batch_update_findings(rh, scan_records)

                      else:
                          logger.info("DB: No Wild card suppression rules for Inspector")


              else:
                  logger.error("Id and/or product name is empty")

              return {
                  'statusCode': 200
              }
      Role: !GetAtt
        - rLambdaExecutionRole
        - Arn
      Description: Suppression lambda for Security Hub implements batch update findings
      FunctionName: !Ref pLambdaName
      Handler: index.handler
      Runtime: python3.12
      Timeout: 300
      Environment:
        Variables:
          eSecHubSuppressTableName: !Ref pSuppressionRuleDynamodbTableName
          eSecHubSuppressHashKey: "id"
          eSecHubSuppressHashRange: "ser_id"
      Tags:
        - Key: !Ref pTagKey1
          Value: !Ref pTagValue1
        - Key: !Ref pTagApplicationKey
          Value: !Ref pTagApplicationValue
        - Key: !Ref pTagOwnerKey
          Value: !Ref pTagOwnerValue
        - Key: !Ref pTagTechnicalContactKey
          Value: !Ref pTagTechnicalContactValue
        - Key: !Ref pTagProductKey
          Value: !Ref pTagProductValue
        - Key: !Ref pTagEnvKey
          Value: !Ref pTagEnvValue
        - Key: !Ref pTagCreatedByKey
          Value: !Ref pTagCreatedByValue
        - Key: !Ref pTagCostCenterIdKey
          Value: !Ref pTagCostCenterIdValue
  rLambdaExecutionPolicy:
    Type: "AWS::IAM::ManagedPolicy"
    Properties:
      PolicyDocument:
        Statement:
          - Action:
              - "logs:CreateLogGroup"
              - "logs:CreateLogStream"
              - "logs:PutLogEvents"
            Effect: Allow
            Resource:
              - !Sub "arn:aws:logs:${AWS::Region}:${AWS::AccountId}:log-group:/aws/lambda/${pLambdaName}:*"
              - !Sub "arn:aws:logs:${AWS::Region}:${AWS::AccountId}:log-group:/aws/lambda/${pSuppressionEngineLambdaName}:*"
            Sid: CloudWatchLogs
          - Action:
              - securityhub:BatchUpdateFindings
              - securityhub:GetFindings
            Effect: Allow
            Resource: "*"
            Sid: SecurityHubBatchUpdateFindings
          - Action:
              - dynamodb:DescribeTable
              - dynamodb:GetItem
              - dynamodb:Scan
              - dynamodb:Query
              - dynamodb:BatchGetItem
              - dynamodb:DescribeTimeToLive
            Effect: Allow
            Resource: !Join
              - ""
              - - "arn:aws:dynamodb:*:"
                - !Ref "AWS::AccountId"
                - !Sub ":table/${pSuppressionRuleDynamodbTableName}"
            Sid: DynamoDBGet
          - Action:
              - dynamodb:Scan
              - dynamodb:Query
            Effect: Allow
            Resource: !Join
              - ""
              - - "arn:aws:dynamodb:*:"
                - !Ref "AWS::AccountId"
                - !Sub ":table/sw-securityhub-suppression-ser-db-v2-main-aws/index/*"
            Sid: DynamoDBScan

          - Action:
              - dynamodb:BatchGetItem
              - dynamodb:BatchWriteItem
              - dynamodb:PutItem
              - dynamodb:DeleteItem
              - dynamodb:GetItem
              - dynamodb:Scan
              - dynamodb:Query
              - dynamodb:UpdateItem
            Effect: Allow
            Resource: !Join
              - ""
              - - "arn:aws:dynamodb:*:"
                - !Ref "AWS::AccountId"
                - !Sub ":table/${pSuppressionEngineCacheDBName}"
            Sid: CacheDbPermissions

          - Action:
              - ssm:PutParameter
              - ssm:GetParameters
              - ssm:GetParameter
            Effect: Allow
            Resource: !Join
              - ""
              - - "arn:aws:ssm:*:"
                - !Ref "AWS::AccountId"
                - !Sub ":parameter${pSupportedRegionsSsmParameterName}"
            Sid: SupportedRegionsSsmPermissions

        Version: 2012-10-17
      Description: Suppression policy for Security Hub implementing batch update findings
      Path: /

  rSecuritySuppressionRule:
    Type: "AWS::Events::Rule"
    Properties:
      Name: sw-event-secrityhub-suppression-rule-main-aws
      Description: SecurityHub Suppression rule
      EventPattern:
        detail:
          findings:
            Compliance:
              Status:
                - FAILED
            Workflow:
              Status:
                - NEW
        detail-type:
          - Security Hub Findings - Imported
        source:
          - aws.securityhub
      State: ENABLED
      Targets:
        - Arn: !GetAtt
            - rSecHubFunction
            - Arn
          Id: "sw-sechub-suppression-target-main-aws"

  rInspectorSuppressionRule:
    Type: "AWS::Events::Rule"
    Properties:
      Name: sw-secruityhub-inspector-events
      Description: Inspector Findings Suppression rule
      State: ENABLED
      EventPattern:
        source:
          - aws.securityhub
        detail-type:
          - Security Hub Findings - Imported
        detail:
          findings:
            Workflow:
              Status:
                - NEW
            GeneratorId:
              - AWSInspector
      Targets:
        - Arn: !GetAtt
            - rSecHubFunction
            - Arn
          Id: "sw-inspector-suppression-target-main-aws"

  rLambdaLogGroup:
    Type: "AWS::Logs::LogGroup"
    Properties:
      LogGroupName: !Sub "/aws/lambda/${rSecHubFunction}"
      RetentionInDays: "30"

  rLambdaPermission:
    Type: "AWS::Lambda::Permission"
    Properties:
      Action: "lambda:InvokeFunction"
      FunctionName: !Ref rSecHubFunction
      Principal: events.amazonaws.com
      SourceArn:
        Fn::GetAtt:
          - "rSecuritySuppressionRule"
          - "Arn"

  rLambdaPermissionInspectorRule:
    Type: "AWS::Lambda::Permission"
    Properties:
      Action: "lambda:InvokeFunction"
      FunctionName: !Ref rSecHubFunction
      Principal: events.amazonaws.com
      SourceArn:
        Fn::GetAtt:
          - "rInspectorSuppressionRule"
          - "Arn"


  ## Reset Lambda and resources
#  rResetLambdaExecutionPolicy:
#    Type: "AWS::IAM::ManagedPolicy"
#    Properties:
#      PolicyDocument:
#        Statement:
#          - Action:
#              - "logs:CreateLogStream"
#              - "logs:PutLogEvents"
#            Effect: Allow
#            Resource: !Join
#              - ""
#              - - "arn:aws:logs:"
#                - !Ref "AWS::Region"
#                - ":"
#                - !Ref "AWS::AccountId"
#                - !Sub ":log-group:/aws/lambda/${pResetLambdaName}:*"
#            Sid: CloudWatchLogs
#          - Action:
#              - securityhub:BatchUpdateFindings
#              - securityhub:GetFindings
#            Effect: Allow
#            Resource: "*"
#            Sid: SecurityHubBatchUpdateFindings
#          - Action:
#              - dynamodb:GetShardIterator
#              - dynamodb:DescribeStream
#              - dynamodb:GetRecords
#            Effect: Allow
#            Resource: !Sub "arn:aws:dynamodb:${AWS::Region}:${AWS::AccountId}:table/${pSuppressionRuleDynamodbTableName}/stream/*"
#            Sid: DynamoDBStreams
#          - Action: dynamodb:ListStreams
#            Resource: "*"
#            Effect: Allow
#            Sid: DynamoDBListStreams
#          - Action: "sqs:SendMessage"
#            Resource: !Sub "arn:aws:sqs:*:${AWS::AccountId}:*"
#            Effect: Allow
#            Sid: SQSDLQPermission
#        Version: 2012-10-17
#      Description: Policy to update and trigger lambda to reset the Suppression for Security Hub Findings
#      Path: /
#
#
#  rResetLambdaExecutionRole:
#    Type: "AWS::IAM::Role"
#    Properties:
#      RoleName: !Sub ${pResetLambdaExecutionRoleName}
#      AssumeRolePolicyDocument:
#        Statement:
#          - Action: "sts:AssumeRole"
#            Effect: Allow
#            Principal:
#              Service: lambda.amazonaws.com
#            Condition:
#              StringEquals:
#                'aws:SourceAccount': !Sub "${AWS::AccountId}"
#        Version: 2012-10-17
#      Description: Suppression role for Security Hub batch update findings
#      ManagedPolicyArns:
#        - !Ref rResetLambdaExecutionPolicy
#      Path: /
#
#  rResetLambdaLogGroup:
#    Type: "AWS::Logs::LogGroup"
#    Properties:
#      LogGroupName: !Sub "/aws/lambda/${pResetLambdaName}"
#      RetentionInDays: 30
#
#  rSecHubResetFunction:
#    Type: "AWS::Lambda::Function"
#    Properties:
#      Code:
#        ZipFile: |
#          import boto3
#          import botocore.exceptions as boto3exceptions
#          import logging
#
#          logger = logging.getLogger()
#          logger.setLevel(logging.INFO)
#
#          client = boto3.client("securityhub")
#
#
#          def handler(event, context):
#            if 'Records' in event:
#              # logger.info(f"Incoming Dynamodb event : {event['Records']}")
#              counter = 0
#              for record in event['Records']:
#                if 'eventName' in record:
#                  if (record['eventName'] == 'MODIFY' or record['eventName'] == 'INSERT' or record[
#                  'eventName'] == 'DELETE') \
#                    and record['eventSource'] == 'aws:dynamodb':
#                    security_control_id = record["dynamodb"]["Keys"]["id"]["S"]
#                    print(f"security_control_id {security_control_id}")
#                    paginator = client.get_paginator("get_findings")
#                    page_iterator = paginator.paginate(Filters={
#                      'ComplianceSecurityControlId': [
#                        {
#                          'Value': security_control_id,
#                          'Comparison': 'EQUALS'
#                        }
#                      ],
#                      'RecordState': [
#                        {
#                          'Value': "ACTIVE",
#                          'Comparison': 'EQUALS'
#                        }
#                      ],
#                      'ComplianceStatus': [
#                        {
#                          'Value': "FAILED",
#                          'Comparison': 'EQUALS'
#                        }
#                      ]
#                    })
#
#                    for page in page_iterator:
#                      for finding in page["Findings"]:
#                        # logger.info(f"finding {finding}")
#                        finding_id = finding["Id"]
#                        product_arn = finding["ProductArn"]
#                        counter = counter + 1
#                        try:
#                          response = client.batch_update_findings(
#                            FindingIdentifiers=[
#                              {
#                                'Id': finding_id,
#                                'ProductArn': product_arn
#                              },
#                            ],
#                            Workflow={
#                              'Status': 'NEW'
#                            }
#                          )
#                          if response["ProcessedFindings"]:
#                            logger.info(f"ProcessedFindings {response['ProcessedFindings']}")
#                          elif response["UnprocessedFindings"]:
#                            logger.info(f"UnprocessedFindings {response['UnprocessedFindings']}")
#                        except boto3exceptions.ClientError as error:
#                          logger.exception("client error")
#                          raise ConnectionError(f"Client error invoking batch update findings {error}")
#                        except boto3exceptions.ParamValidationError as error:
#                          logger.exception("invalid parameters")
#                          raise ValueError(f"The parameters you provided are incorrect: {error}")
#
#              print(f"Total Findings: {counter}")
#
#            return {
#                'statusCode': 200
#            }
#      Role: !GetAtt
#        - rResetLambdaExecutionRole
#        - Arn
#      Description: The Lambda function to reset the Workflow status of SecurityHub findings when an item is created/updated/deleted in the CSCI DynamoDB table.
#      FunctionName: !Ref pResetLambdaName
#      Handler: index.handler
#      Runtime: python3.12
#      Timeout: 900
#      Tags:
#        - Key: !Ref pTagKey1
#          Value: !Ref pTagValue1
#        - Key: !Ref pTagApplicationKey
#          Value: !Ref pTagApplicationValue
#        - Key: !Ref pTagOwnerKey
#          Value: !Ref pTagOwnerValue
#        - Key: !Ref pTagTechnicalContactKey
#          Value: !Ref pTagTechnicalContactValue
#        - Key: !Ref pTagProductKey
#          Value: !Ref pTagProductValue
#        - Key: !Ref pTagEnvKey
#          Value: !Ref pTagEnvValue
#        - Key: !Ref pTagCreatedByKey
#          Value: !Ref pTagCreatedByValue
#        - Key: !Ref pTagCostCenterIdKey
#          Value: !Ref pTagCostCenterIdValue
#
#  rResetLambdaSQSOnFailureDLQ:
#    Type: AWS::SQS::Queue
#    Properties:
#      DelaySeconds: 0
#      MaximumMessageSize: 262144
#      MessageRetentionPeriod: 604800
#      QueueName: sw-sqs-lambda-suppression-reset-failures
#      ReceiveMessageWaitTimeSeconds: 1
#      VisibilityTimeout: 30
#
#  rDynamoDBLambdaEventSourceMapping:
#    Type: AWS::Lambda::EventSourceMapping
#    Properties:
#      BatchSize: 1
#      BisectBatchOnFunctionError: false
#      DestinationConfig:
#        OnFailure:
#          Destination: !GetAtt rResetLambdaSQSOnFailureDLQ.Arn
#      Enabled: true
#      EventSourceArn: !Ref pSuppressionRuleDynamoDbStreamArn
#      FunctionName: !GetAtt rSecHubResetFunction.Arn
#      FunctionResponseTypes:
#        - ReportBatchItemFailures
#      MaximumBatchingWindowInSeconds: 0
#      MaximumRecordAgeInSeconds: -1
#      MaximumRetryAttempts: 1
#      ParallelizationFactor: 1
#      StartingPosition: LATEST

  ## CCOE-8097 : New implementation of Suppression rule engine to handle following requirements:
  ##  1. Process findings in batch of 100 using cache db
  ##  2. Process historic findings
  ##  3. Handle rule update/delete

  # Cache (Dynamodb) to store the findings along with the rule against which it matched by suppression engine.
  rSuppressionEngineCacheDB:
    Type: "AWS::DynamoDB::Table"
    Properties:
      TableName: !Ref pSuppressionEngineCacheDBName
      BillingMode: PAY_PER_REQUEST
      PointInTimeRecoverySpecification:
        PointInTimeRecoveryEnabled: true
      AttributeDefinitions:
        - AttributeName: FindingId
          AttributeType: S
        - AttributeName: RuleId
          AttributeType: S
      KeySchema:
        - AttributeName: FindingId
          KeyType: HASH
        - AttributeName: RuleId
          KeyType: RANGE

  # Suppression engine triggers

  # 1. Scheduler to trigger engine to process cached findings
  rSuppressionEngineCurrentScheduler:
    Type: AWS::Events::Rule
    Properties:
      Description: Trigger suppression engine to process cached findings
      ScheduleExpression: rate(10 minutes)
      Name: "sw-suppression-engine-process-current-main-aws"
      State: ENABLED
      Targets:
        - Arn: !GetAtt "rSuppressionEngine.Arn"
          Id: suppression_engine_current
          Input: |-
            {
              "scheduler-type": "current"
            }

  # 2. Scheduler to trigger engine to process historic findings which are in NEW state
  rSuppressionEngineHistoricNewScheduler:
    Type: AWS::Events::Rule
    Properties:
      Description: Trigger suppression engine to process historic findings which are in NEW state
      ScheduleExpression: rate(6 hours)
      Name: "sw-suppression-engine-process-historic-new-main-aws"
      State: ENABLED
      Targets:
        - Arn: !GetAtt "rSuppressionEngine.Arn"
          Id: suppression_engine_historic
          Input: |-
            {
              "scheduler-type": "historic-new"
            }

  # 3. Scheduler to trigger engine to process historic findings which are in SUPPRESSED state
  rSuppressionEngineHistoricSuppressedScheduler:
    Type: AWS::Events::Rule
    Properties:
      Description: Trigger suppression engine to process historic findings which are in SUPPRESSED state
      ScheduleExpression: rate(6 hours)
      Name: "sw-suppression-engine-process-historic-suppressed-main-aws"
      State: ENABLED
      Targets:
        - Arn: !GetAtt "rSuppressionEngine.Arn"
          Id: suppression_engine_historic
          Input: |-
            {
              "scheduler-type": "historic-suppressed"
            }


  # 4. Trigger on security hub findings
  rSuppressionEngineSecurityHubTrigger:
    Type: "AWS::Events::Rule"
    Properties:
      Name: sw-event-secrityhub-suppression-rule-v2-main-aws
      Description: SecurityHub Suppression rule
      EventPattern:
        detail:
          findings:
            Compliance:
              Status:
                - FAILED
            Workflow:
              Status:
                - NEW
        detail-type:
          - Security Hub Findings - Imported
        source:
          - aws.securityhub
      State: ENABLED
      Targets:
        - Arn: !GetAtt
            - rSuppressionEngine
            - Arn
          Id: "sw-sechub-suppression-target-v2-main-aws"

  # 5. Trigger on inspector findings
  rSuppressionEngineInspectorTrigger:
    Type: "AWS::Events::Rule"
    Properties:
      Name: sw-secruityhub-inspector-events-v2
      Description: Inspector Findings Suppression rule
      State: ENABLED
      EventPattern:
        source:
          - aws.securityhub
        detail-type:
          - Security Hub Findings - Imported
        detail:
          findings:
            Workflow:
              Status:
                - NEW
            GeneratorId:
              - AWSInspector
      Targets:
        - Arn: !GetAtt
            - rSuppressionEngine
            - Arn
          Id: "sw-inspector-suppression-target-v2-main-aws"

  # SSM parameter containing list of supported regions
  rSuppressionEngineSupportedRegions:
    Type: AWS::SSM::Parameter
    Properties:
      Name: !Sub ${pSupportedRegionsSsmParameterName}
      Type: String
      Value: !Sub ${pSupportedRegions}
      Description: List of all supported regions managed by suppression engine

  # Suppression engine impl
  rSuppressionEngineLogGroup:
    Type: "AWS::Logs::LogGroup"
    Properties:
      LogGroupName: !Sub "/aws/lambda/${rSuppressionEngine}"
      RetentionInDays: "30"


  rSuppressionEnginePermissionCurrent:
    Type: AWS::Lambda::Permission
    Properties:
      Action: lambda:InvokeFunction
      FunctionName: !Ref rSuppressionEngine
      Principal: events.amazonaws.com
      SourceArn: !GetAtt rSuppressionEngineCurrentScheduler.Arn

  rSuppressionEnginePermissionHistoricNew:
    Type: AWS::Lambda::Permission
    Properties:
      Action: lambda:InvokeFunction
      FunctionName: !Ref rSuppressionEngine
      Principal: events.amazonaws.com
      SourceArn: !GetAtt rSuppressionEngineHistoricNewScheduler.Arn

  rSuppressionEnginePermissionHistoricSuppressed:
    Type: AWS::Lambda::Permission
    Properties:
      Action: lambda:InvokeFunction
      FunctionName: !Ref rSuppressionEngine
      Principal: events.amazonaws.com
      SourceArn: !GetAtt rSuppressionEngineHistoricSuppressedScheduler.Arn

  rSuppressionEnginePermissionSecurityHub:
    Type: AWS::Lambda::Permission
    Properties:
      Action: lambda:InvokeFunction
      FunctionName: !Ref rSuppressionEngine
      Principal: events.amazonaws.com
      SourceArn: !GetAtt rSuppressionEngineSecurityHubTrigger.Arn

  rSuppressionEnginePermissionInspector:
    Type: AWS::Lambda::Permission
    Properties:
      Action: lambda:InvokeFunction
      FunctionName: !Ref rSuppressionEngine
      Principal: events.amazonaws.com
      SourceArn: !GetAtt rSuppressionEngineInspectorTrigger.Arn


  rSuppressionEngine:
    Type: AWS::Lambda::Function
    Properties:
      Handler: index.lambda_handler
      FunctionName: !Ref pSuppressionEngineLambdaName
      Description: !Sub "Suppression engine which evaluates the security hub findings against defined rules in {pSuppressionRuleDynamodbTableName} db and process them based if a rule is matched."
      Runtime: python3.13
      Role: !GetAtt
        - rLambdaExecutionRole
        - Arn
      Timeout: 900
      Environment:
        Variables:
          eSecHubSuppressTableName: !Ref pSuppressionRuleDynamodbTableName
          eSecHubSuppressHashKey: "id"
          eSecHubSuppressHashRange: "ser_id"
          eSecHubSuppressCache: !Ref pSuppressionEngineCacheDBName
          EXECUTION_MODE: "execute"
          SUPPORTED_REGIONS_SSM_PARAMETER: !Sub ${pSupportedRegionsSsmParameterName}
          LOGGING_LEVEL: "INFO"
      Tags:
        - Key: !Ref pTagKey1
          Value: !Ref pTagValue1
        - Key: !Ref pTagApplicationKey
          Value: !Ref pTagApplicationValue
        - Key: !Ref pTagOwnerKey
          Value: !Ref pTagOwnerValue
        - Key: !Ref pTagTechnicalContactKey
          Value: !Ref pTagTechnicalContactValue
        - Key: !Ref pTagProductKey
          Value: !Ref pTagProductValue
        - Key: !Ref pTagEnvKey
          Value: !Ref pTagEnvValue
        - Key: !Ref pTagCreatedByKey
          Value: !Ref pTagCreatedByValue
        - Key: !Ref pTagCostCenterIdKey
          Value: !Ref pTagCostCenterIdValue
      Code:
        ZipFile: |

          import logging
          import os
          import re
          from datetime import datetime, timezone
          from typing import List, Dict, Optional
          from boto3.dynamodb.conditions import Key, Attr
          import boto3

          logger = logging.getLogger()
          RULES_TABLE_NAME = os.environ['eSecHubSuppressTableName']
          CACHE_TABLE_NAME = os.environ['eSecHubSuppressCache']
          EXECUTION_MODE = os.environ.get("EXECUTION_MODE", "log").lower()  # 'execute' or 'log'

          # Read log level from env var, default to INFO
          LOGGING_LEVEL = os.getenv("LOGGING_LEVEL", "INFO").upper()
          if LOGGING_LEVEL not in ["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"]:
            LOGGING_LEVEL = "INFO"
          logging.basicConfig(level=getattr(logging, LOGGING_LEVEL))
          logger = logging.getLogger(__name__)
          logger.setLevel(getattr(logging, LOGGING_LEVEL))

          dynamodb = boto3.resource('dynamodb')
          securityhub = boto3.client('securityhub')
          ssm = boto3.client("ssm")

          class SecurityHubFinding:

            def __init__(self, payload):
              self.product_name = (
                payload.get("ProductName") or
                payload.get("ProductFields", {}).get("aws/securityhub/ProductName") or
                "UnknownProduct"
              )
              self.finding_identifiers = []
              self.title = payload["Title"]
              self.label = payload["Severity"]["Label"]
              self.resource_id = ''
              self.finding_account_id = payload["AwsAccountId"]
              self.extra_resource_arn = ''
              self.id = ''
              self.generator_id = payload["GeneratorId"]
              self.resource_type = ''
              self.workflow_status = payload["Workflow"]["Status"]

              if self.product_name == "Security Hub" and 'Compliance' in payload and 'SecurityControlId' in payload["Compliance"]:
                self.id = payload["Compliance"]["SecurityControlId"]
              elif self.product_name == "Inspector" and 'Vulnerabilities' in payload and 'Id' in payload["Vulnerabilities"][0]:
                self.id = payload["Vulnerabilities"][0]["Id"]
              elif self.product_name in ("IAM Access Analyzer", "Health", "GuardDuty") and 'Resources' in payload and 'Id' in payload["Resources"]:
                self.id = payload["Resources"]["Id"]
              resource_one = payload["Resources"][0]
              finding_identifier = {
                "Id": payload["Id"],
                "ProductArn": payload["ProductArn"],
              }
              self.finding_identifiers.append(finding_identifier)
              self.finding_account_id = payload["AwsAccountId"]
              self.resource_id = resource_one["Id"]
              self.resource_type = resource_one["Type"]
              if "Details" in resource_one and "AwsEc2Instance" in resource_one["Details"] and "IamInstanceProfileArn" in resource_one["Details"]["AwsEc2Instance"]:
                self.extra_resource_arn = resource_one["Details"]["AwsEc2Instance"]["IamInstanceProfileArn"]
              elif "Details" in resource_one and "AwsEc2SecurityGroup" in resource_one["Details"] and "GroupName" in resource_one["Details"]["AwsEc2SecurityGroup"]:
                self.extra_resource_arn = resource_one["Details"]["AwsEc2SecurityGroup"]["GroupName"]
              elif "Details" in resource_one and "AwsEc2Instance" in resource_one["Details"] and "KeyName" in resource_one["Details"]["AwsEc2Instance"]:
                self.extra_resource_arn = resource_one["Details"]["AwsEc2Instance"]["KeyName"]
              self.note = payload.get("Note")
              self.udf = payload.get("UserDefinedFields", {})

            def __str__(self):
              return (
                  f"SecurityHubFinding("
                  f"id={self.id}, "
                  f"product_name={self.product_name}, "
                  f"title={self.title!r}, "
                  f"severity={self.label}, "
                  f"resource_id={self.resource_id}, "
                  f"resource_type={self.resource_type}, "
                  f"status={self.workflow_status}, "
                  f"finding_identifiers={self.finding_identifiers}, "
                  f"udf={self.udf}, "
                  f"notes={self.note})"
              )

            @staticmethod
            def from_api_finding(finding: dict) -> "SecurityHubFinding":
              return SecurityHubFinding(finding)

            def to_dict(self):
              return {
                  "id": self.id,
                  "product_name": self.product_name,
                  "title": self.title,
                  "label": self.label,
                  "resource_id": self.resource_id,
                  "finding_account_id": self.finding_account_id,
                  "extra_resource_arn": self.extra_resource_arn,
                  "generator_id": self.generator_id,
                  "resource_type": self.resource_type,
                  "workflow_status": self.workflow_status,
                  "finding_identifiers": self.finding_identifiers,
                  "notes": self.note,
                  "udf": self.udf
            }

          class Rule:
            def __init__(self, rule_data: Dict):
              self.id = rule_data['id']
              self.product_name = rule_data.get('product_name')
              self.ser_id = rule_data.get('ser_id')
              self.ser_link = rule_data.get('ser_link')
              self.account_exception = [a.strip() for a in rule_data.get('account_exception', '').split(',') if a.strip()]
              self.account_inclusion = [a.strip() for a in rule_data.get('account_inclusion', '').split(',') if a.strip()]
              self.resource_type = rule_data.get('resource_type', '')
              self.finding_title = rule_data.get('finding_title', '')
              self.resource_pattern = rule_data.get('resource_pattern', '')
              self.extra_resource_pattern = rule_data.get('extra_resource_pattern', '')
              self.from_severity = rule_data.get('from_severity', '')
              self.to_severity = rule_data.get('to_severity', '')
              self.due_date = self._parse_due_date(rule_data.get('due_date'))
              self.valid_rule = self._validate_rule()

            def _validate_rule(self):
              if self.product_name == "Inspector":
                # ResourcePattern + ID cross-validation
                trimmed_id = self.id.strip() if self.id else ""
                if len(self.resource_pattern) > 0:
                  # L2: Block wildcards in resource_pattern when ID is *
                  if trimmed_id == "*" and "*" in self.resource_pattern:
                    logger.error(f"Suppression rule (Id - {self.id}) has wildcard in resource_pattern when ID is * - {self.resource_pattern}")
                    return False
                  # L1: Allow wildcards in resource_pattern when ID is specific - PASS
                
                # L5: Block wildcards in resource_type ALWAYS (original behavior)
                if len(self.resource_type) > 0 and "*" in self.resource_type:
                  logger.error(f"Suppression rule (Id - {self.id}) has wildcard in resource_type - {self.resource_type}")
                  return False                
                if len(self.resource_pattern) == 0 and len(self.resource_type) == 0:
                  logger.error(f"Please provide both or one of ResourceType or ResourcePattern or Id")
                  return False
              elif len(self.resource_pattern) == 0 and len(self.resource_type) == 0:
                  return False
              return True

            def _parse_due_date(self, due_date: Optional[str]) -> Optional[datetime]:
              if not due_date:
                return None
              for fmt in ('%Y-%m-%d', '%m/%d/%Y', '%Y/%d/%m'):
                try:
                  return datetime.strptime(due_date, fmt).replace(tzinfo=timezone.utc)
                except ValueError:
                  continue
              logger.error(f"Failed to parse due date '{due_date}' with supported formats.")
              return None

            def is_expired(self) -> bool:
              return self.due_date and datetime.now(timezone.utc).date() > self.due_date.date()

            def matches(self, finding: SecurityHubFinding) -> Optional[Dict]:
              id_match = False
              if finding.id == self.id:
                id_match = True
                logger.debug(f"Suppression rule has matched ID: {finding.id} and Rule Id - {self.id}")
              else:
                if self.product_name == "Inspector":
                  id_prefix = re.compile(r'^' + self.id.strip("*"))
                  if id_prefix.search(finding.id):
                    id_match = True
                    logger.debug(f"Suppression rule has matched ID: {finding.id} and Vulnerability Id - {self.id}")
              is_resource_id_type_match = self._match_resource_id_type(finding)
              if id_match and \
                      is_resource_id_type_match and \
                      ((self.account_inclusion and finding.finding_account_id in self.account_inclusion) or \
                       (not self.account_inclusion and finding.finding_account_id not in self.account_exception)) and \
                      re.search(self.extra_resource_pattern, finding.extra_resource_arn):
                logger.debug(f"Finding event matched the rule id {self.id}")
                if self.from_severity and self.to_severity and finding.label == self.from_severity:
                  return {"action": "severity_update", "new_severity": self.to_severity}
                return {"action": "suppress", "new_severity": None}
              return None

            def _match_resource_id_type(self, finding: SecurityHubFinding):
              if len(self.resource_pattern) > 0 and len(self.resource_type) > 0:
                if re.search(self.resource_pattern, finding.resource_id) and re.search(self.resource_type,finding.resource_type):
                  logger.debug(f"Suppression rule matched - Resource Id and Resource Type")
                  return True
                else:
                  logger.debug(f"Suppression rule not matched - DB Resource Id: {self.resource_pattern} and DB Resource Type: {self.resource_type}")
                  return False
              elif len(self.resource_pattern) > 0 and re.search(self.resource_pattern, finding.resource_id):
                logger.debug(f"Suppression rule  matched - Resource Id")
                return True
              elif len(self.resource_type) > 0 and re.search(self.resource_type, finding.resource_type):
                logger.debug(f"Suppression rule matched - Resource Type")
                return True
              else:
                logger.debug(f"Suppression rule not matched - Either Resource Id or  Resource Type is empty")
                return False

          class SuppressionRuleEngine:
            def __init__(self):
              self.rules_table = dynamodb.Table(RULES_TABLE_NAME)
              self.cache_table = dynamodb.Table(CACHE_TABLE_NAME)
              self.rules: List[Rule] = self.load_rules()
              self.rule_ids = {rule.id for rule in self.rules}

              self.supported_regions_ssm_param_name = os.environ.get("SUPPORTED_REGIONS_SSM_PARAMETER","/security-tools/suppression-engine/supported-regions")
              # --- Load supported regions from ssm ---
              self.supported_regions = self._get_supported_regions()
              if not self.supported_regions:
                  raise ValueError("SUPPORTED_REGIONS var is empty")

            def _get_supported_regions(self) -> list[str]:
              try:
                response = ssm.get_parameter(Name=self.supported_regions_ssm_param_name)
                regions = response["Parameter"]["Value"].split(",")
                return [r.strip() for r in regions if r.strip()]
              except Exception as e:
                logger.error(f"Failed to read ssm param. Error : {e}")

            def rotate_supported_regions(self):
              if self.supported_regions:
                first = self.supported_regions.pop(0)
                self.supported_regions.append(first)
                ssm.put_parameter(
                    Name=self.supported_regions_ssm_param_name,
                    Value=",".join(self.supported_regions),
                    Type="String",
                    Overwrite=True
                )
                logger.debug(f"Rotated regions with new order as : {self.supported_regions}")

            def _batch_update_findings(self, **kwargs):
              if EXECUTION_MODE == "execute":
                return securityhub.batch_update_findings(**kwargs)
              else:
                logger.info(f"[LOG MODE] Would call batch_update_findings with: {kwargs}")
                return {"ProcessedFindings": [], "UnprocessedFindings": []}

            def load_rules(self) -> List[Rule]:
              response = self.rules_table.scan()
              rules = [Rule(item) for item in response['Items']]
              logger.info(f"Loaded {len(rules)} suppression rules.")
              return rules

            def cache_eligible_for_suppression_finding(
                        self,
                        finding: SecurityHubFinding,
                        event_type: str = "current",
                        old_status: str = "NEW",
                        new_status: str = "SUPPRESSED"):
              logger.info(f"Checking if eligible for {event_type} event with finding as {finding}")
              finding_id_arn = finding.finding_identifiers[0]['Id']
              for rule in self.rules:
                if rule.is_expired():
                  logger.info(f"rule {rule.id} is expired so skipping match.")
                  continue
                if not rule.valid_rule:
                  logger.info(f"rule {rule.id} is not valid so skipping match.")
                  continue
                match_result = rule.matches(finding)
                if match_result:
                  item = {
                      'FindingId': finding_id_arn,
                      'RuleId': rule.id,
                      'SerId': rule.ser_id,
                      'SerLink': rule.ser_link,
                      'Finding': finding.to_dict(),
                      'Action': match_result["action"],
                      'OldStatus': old_status,
                      'NewStatus': new_status
                  }
                  if match_result["action"] == "severity_update":
                    item["NewSeverity"] = match_result["new_severity"]
                  if rule.due_date:
                    item["SerDueDate"] = rule.due_date.strftime('%m/%d/%Y')
                  if event_type == "historic" and old_status == "SUPPRESSED" and finding.udf and 'RuleId' in finding.udf:
                    if finding.udf['RuleId'] != rule.id:
                      self.cache_table.put_item(Item=item)
                      logger.info(f"Caching historic finding {finding_id_arn} matched with rule {rule.id} which is eligible for re-suppression.")
                      return
                    else:
                      logger.info(f"Historic finding {finding_id_arn} matched with rule {rule.id} but was already suppressed with this rule so skipping.")
                      return
                  else:
                    self.cache_table.put_item(Item=item)
                    logger.info(f"Caching finding {finding_id_arn} matched with rule {rule.id} which is eligible for suppression.")
                    return
                else:
                  logger.info(f"No match found for rule {rule.id} for finding : {finding}.")

              if event_type == "historic" and old_status == "SUPPRESSED":
                if finding.udf and 'RuleId' in finding.udf:
                  item = {
                      'FindingId': finding_id_arn,
                      'RuleId': finding.udf['RuleId'],
                      'SerId': finding.udf['SerId'],
                      'SerLink': finding.udf['SerLink'],
                      'Finding': finding.to_dict(),
                      'Action': "unsuppress",
                      'OldStatus': old_status,
                      'NewStatus': new_status
                  }
                  self.cache_table.put_item(Item=item)
                  logger.info(f"Historic finding {finding_id_arn} and now has no rule matched so eligible for un-suppress.")
              else:
                logger.info(f"No rule matched for {finding_id_arn} so it is not eligible for suppression.")


            def process_findings_from_cache_for_deleted_rules(self):
              response = self.cache_table.scan()
              items = response.get('Items', [])
              if not items:
                return
              deleted_rule_items = [item for item in items if item.get("RuleId") not in self.rule_ids]
              if not deleted_rule_items:
                return
              logger.info(f"Found {len(deleted_rule_items)} cached findings tied to deleted rules.")
              for i in range(0, len(deleted_rule_items), 100):
                chunk = deleted_rule_items[i:i + 100]
                identifiers = []
                ident_to_cachekeys = {}
                for f in chunk:
                  for ident in f["Finding"]["finding_identifiers"]:
                    tup = (ident["Id"], ident["ProductArn"])
                    identifiers.append({"Id": tup[0], "ProductArn": tup[1]})
                    ident_to_cachekeys[tup] = {"FindingId": f["FindingId"], "RuleId": f["RuleId"]}

                if not identifiers:
                  continue

                # Get deleted RuleId from cache
                deleted_rule_id = chunk[0].get("RuleId", "unknown-rule")

                # Build audit note with timestamp
                now_str = datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%SZ")
                new_note_text = f"[{now_str}] RuleId={deleted_rule_id} deleted | Action=unsuppress"

                first_item = chunk[0]
                existing_note = first_item["Finding"].get("notes", {})
                if existing_note and isinstance(existing_note, dict) and "Text" in existing_note:
                  combined_text = f"{existing_note['Text']}\n---\n{new_note_text}"
                else:
                  combined_text = new_note_text

                # Preserve UDF and add deleted markers
                existing_udf = first_item["Finding"].get("udf", {}) or {}
                udf = dict(existing_udf)
                udf["Reason"] = "deleted"
                udf["DeletedRuleId"] = deleted_rule_id


                update_kwargs = {
                        "FindingIdentifiers": identifiers,
                        "Workflow": {"Status": "NEW"},
                        "Note": {
                            "Text": combined_text,
                            "UpdatedBy": "SuppressionRuleEngine"
                        },
                        "UserDefinedFields": {"RuleId": "deleted"}
                    }

                logger.info(f"Unsuppressing {len(identifiers)} findings due to deleted RuleId={deleted_rule_id}...")

                try:
                  resp = self._batch_update_findings(**update_kwargs)

                  processed_entries = resp.get("ProcessedFindings", [])
                  processed_ids = set()
                  for p in processed_entries:
                    if "FindingIdentifier" in p:
                      fid = p["FindingIdentifier"]
                      processed_ids.add((fid.get("Id"), fid.get("ProductArn")))
                    else:
                      processed_ids.add((p.get("Id"), p.get("ProductArn")))

                  unprocessed_entries = resp.get("UnprocessedFindings", [])
                  unprocessed_dict = {}
                  for u in unprocessed_entries:
                    if "FindingIdentifier" in u and isinstance(u["FindingIdentifier"], dict):
                      fid = u["FindingIdentifier"]
                      unprocessed_dict[fid.get("Id")] = f"Reason: {u.get('ErrorMessage', 'Unknown')}"
                    else:
                      unprocessed_dict[u.get("FindingIdentifier", {}).get("Id","unknown")] = f"Reason: {u.get('ErrorMessage', 'Unknown')}"

                  # Delete processed entries from cache
                  if processed_ids:
                    with self.cache_table.batch_writer() as writer:
                      for ident_tuple in processed_ids:
                        cache_keys = ident_to_cachekeys.get(ident_tuple)
                        if cache_keys:
                          try:
                            writer.delete_item(Key={"FindingId": cache_keys["FindingId"], "RuleId": cache_keys["RuleId"]})
                            logger.info(f"Deleted cache item for FindingId={cache_keys['FindingId']} RuleId={cache_keys['RuleId']}")
                          except Exception as e:
                            logger.error(f"Failed deleting cache item {cache_keys}: {e}")

                  # Log summary for this chunk
                  if processed_ids or unprocessed_dict:
                    logger.info(f"==============DELETED RULE START SUMMARY====================")
                    logger.info(f"For deleted rules: processed_count={len(processed_ids)} unprocessed_count={len(unprocessed_dict)}")
                    logger.info(f"Processed findings details: {processed_ids}")
                    if unprocessed_dict:
                      logger.error(f"Unprocessed findings details: {unprocessed_dict}")
                    logger.info(f"==============DELETED RULE START SUMMARY END SUMMARY====================")

                except Exception as e:
                  logger.error(f"batch_update_findings failed while unsuppressing deleted-rule findings: {e}")
                  continue

            def process_findings_from_cache(self):
              for rule in self.rules:
                # Query cache by RuleId
                response = self.cache_table.scan(FilterExpression=Attr("RuleId").eq(rule.id),)
                items = response.get("Items", [])
                if not items:
                  logger.debug(f'No findings exists in cache for rule id : {rule.id} so moving on...')
                  continue
                logger.debug(f"Processing {len(items)} cached findings for RuleId={rule.id}")
                # Process in chunks of 100
                for i in range(0, len(items), 100):
                  chunk = items[i:i + 100]
                  identifiers = []
                  ident_to_cachekeys = {}
                  for f in chunk:
                    for ident in f["Finding"]["finding_identifiers"]:
                      tup = (ident["Id"], ident["ProductArn"])
                      identifiers.append({"Id": tup[0], "ProductArn": tup[1]})
                      ident_to_cachekeys[tup] = {"FindingId": f["FindingId"], "RuleId": f["RuleId"]}

                  if not identifiers:
                    logger.info(f"No identifiers found in chunk for RuleId={rule.id}, skipping chunk.")
                    continue

                  # Use action/severity from first item (all items from same rule should match)
                  first_item = chunk[0]
                  action = first_item["Action"]
                  new_severity = first_item.get("NewSeverity")
                  ser_id = first_item.get("SerId")
                  ser_link = first_item.get("SerLink")
                  due_date = first_item.get("SerDueDate")

                  update_kwargs = {"FindingIdentifiers": identifiers}

                  # Workflow / Severity updates
                  if action == "suppress":
                    update_kwargs["Workflow"] = {"Status": "SUPPRESSED"}
                  elif action == "unsuppress":
                    update_kwargs["Workflow"] = {"Status": "NEW"}
                  elif action == "severity_update" and new_severity:
                    update_kwargs["Severity"] = {"Label": new_severity}

                  # Notes
                  notes = [ser_link] if ser_link else []
                  if due_date:
                    notes.append(f"Due Date: {due_date}")
                  current_dt = datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%SZ")
                  new_note_text = f"[{current_dt}] Rule {rule.id} | Action={action} | {' | '.join(notes)}".strip(" |")

                  # Append to existing note if available
                  existing_note = first_item["Finding"].get("notes", {})

                  if existing_note and isinstance(existing_note, dict) and "Text" in existing_note:
                    combined_text = f"{existing_note['Text']}\n---\n{new_note_text}"
                  else:
                    combined_text = new_note_text

                  update_kwargs["Note"] = {
                      "Text": combined_text,
                      "UpdatedBy": "SuppressionRuleEngine"
                  }

                  # UserDefinedFields
                  udf = {"RuleId": rule.id, "SerId": ser_id}
                  if ser_link:
                    udf["SerLink"] = ser_link
                  if due_date:
                    udf["SerDueDate"] = due_date
                  update_kwargs["UserDefinedFields"] = udf

                  # Call SecurityHub
                  ids_to_log = [f["Id"] for f in identifiers]
                  logger.info(f"[Rule {rule.id}] Updating {len(ids_to_log)} findings: {ids_to_log}")
                  try:
                    resp = self._batch_update_findings(**update_kwargs)

                    processed_entries = resp.get("ProcessedFindings", [])
                    processed_ids = set()

                    for p in processed_entries:
                      if "FindingIdentifier" in p and isinstance(p["FindingIdentifier"], dict):
                        fid = p["FindingIdentifier"]
                        processed_ids.add((fid.get("Id"), fid.get("ProductArn")))
                      else:
                        processed_ids.add((p.get("Id"), p.get("ProductArn")))

                    unprocessed_entries = resp.get("UnprocessedFindings", [])
                    unprocessed_dict = {}

                    for u in unprocessed_entries:
                      if "FindingIdentifier" in u and isinstance(u["FindingIdentifier"], dict):
                        fid = u["FindingIdentifier"]
                        unprocessed_dict[fid.get("Id")] = f"Reason: {u.get('ErrorMessage', 'Unknown')}"
                      else:
                        unprocessed_dict[u.get("FindingIdentifier", {}).get("Id","unknown")] = f"Reason: {u.get('ErrorMessage', 'Unknown')}"

                    # Delete only those cache items that were actually processed
                    if processed_ids:
                      with self.cache_table.batch_writer() as writer:
                        for ident_tuple in processed_ids:
                          cache_keys = ident_to_cachekeys.get(ident_tuple)
                          if cache_keys:
                            try:
                              writer.delete_item(Key={"FindingId": cache_keys["FindingId"], "RuleId": cache_keys["RuleId"]})
                              logger.debug(f"Deleted cache item for FindingId={cache_keys['FindingId']} RuleId={cache_keys['RuleId']}")
                            except Exception as e:
                              logger.error(f"Failed deleting cache item for {cache_keys}: {e}")
                          else:
                            logger.warning(f"Processed identifier {ident_tuple} not found in chunk->cache mapping; skipping deletion.")
                    else:
                      logger.info(f"No processed findings returned for RuleId={rule.id} in this chunk.")

                    # Log summary for this chunk
                    if processed_ids or unprocessed_dict:
                      logger.info(f"==============[RULE : {rule.id} ] START SUMMARY====================")
                      logger.info(f"For RuleId={rule.id}: processed_count={len(processed_ids)} unprocessed_count={len(unprocessed_dict)}")
                      logger.info(f"Processed findings details: {processed_ids}")
                      if unprocessed_dict:
                        logger.error(f"Unprocessed findings details: {unprocessed_dict}")
                      logger.info(f"==============[RULE : {rule.id} ] END SUMMARY====================")


                  except Exception as e:
                    logger.error(f"batch_update_findings failed for RuleId={rule.id}: {e}")
                    continue


            def reevaluate_historic_suppressed_findings(self):
              logger.info(f"Will work on suppressed historic findings in {self.supported_regions[0]}")
              paginator = securityhub.get_paginator("get_findings")
              # --- Suppressed findings ---
              try:
                for page_num, page in enumerate(
                        paginator.paginate(
                            Filters={
                                "WorkflowStatus": [{"Value": "SUPPRESSED", "Comparison": "EQUALS"}],
                                "Region": [{"Value": self.supported_regions[0], "Comparison": "EQUALS"}]
                            }
                        ),
                        start=1
                ):
                  findings = page.get("Findings", [])
                  logger.debug(f"Suppressed findings page {page_num}: {len(findings)} findings returned")
                  for finding in findings:
                    sec_hub_finding = SecurityHubFinding(finding)
                    udf = sec_hub_finding.udf
                    rule_id = udf.get("RuleId") if udf else None
                    if not rule_id:
                      logger.info(f"Suppressed finding {sec_hub_finding.id} has no RuleId in UDF so reevaluating")
                    elif rule_id not in self.rule_ids:
                      logger.info(f"Suppressed finding {sec_hub_finding.id} has invalid RuleId={rule_id} so reevaluating")
                    else:
                      rule_obj = next((r for r in self.rules if r.id == rule_id), None)
                      if not rule_obj.is_expired():
                        logger.info(f"Suppressed finding {sec_hub_finding.id} already has current RuleId={rule_id} so skipping")
                        continue
                      else:
                        logger.info(f"Suppressed finding {sec_hub_finding.id} already has current RuleId={rule_id} but now that rule is either expired or invalid so will reevaluate")
                    self.cache_eligible_for_suppression_finding(
                        finding=sec_hub_finding,
                        event_type="historic",
                        old_status="SUPPRESSED",
                        new_status="NEW"
                    )
              except Exception as e:
                logger.error(f'Exception happened while getting finding for SUPPRESSED so stopping further processing. Error : {e}')
                return



            def reevaluate_historic_new_findings(self, context):
              logger.debug(f"Will work on new historic findings in {self.supported_regions[0]}")
              paginator = securityhub.get_paginator("get_findings")
              # --- New findings ---
              try:
                for page_num, page in enumerate(
                        paginator.paginate(
                            Filters={
                                "WorkflowStatus": [{"Value": "NEW", "Comparison": "EQUALS"}],
                                "Region": [{"Value": self.supported_regions[0], "Comparison": "EQUALS"}]
                            }
                        ),
                        start=1
                ):
                  remaining_ms = context.get_remaining_time_in_millis()
                  if remaining_ms < 3 * 60 * 1000:
                    logger.debug(f'Lambda timeout will be in {remaining_ms / 1000}s so returning back from here...')
                    return
                  findings = page.get("Findings", [])
                  logger.debug(f"NEW findings page {page_num}: {len(findings)} findings returned")
                  for finding in findings:
                    sec_hub_finding = SecurityHubFinding(finding)
                    self.cache_eligible_for_suppression_finding(
                        finding=sec_hub_finding,
                        event_type="historic",
                        old_status="NEW",
                        new_status="SUPPRESSED"
                    )
              except Exception as e:
                logger.error(f'Exception happened while getting finding for NEW so stopping further processing. Error : {e}')
                return
          def lambda_handler(event, context):
            logger.info(f"Incoming event : {repr(event)}")
            suppression_engine = SuppressionRuleEngine()
            scheduler_type = event.get("scheduler-type")

            # --- Case 2: New Security Hub finding ---
            if event.get("source") == "aws.securityhub":
              logger.info("Event type : SecurityHub")
              findings = event.get("detail", {}).get("findings", [])
              for finding in findings:
                sec_hub_finding = SecurityHubFinding(finding)
                logger.info(f"Transformed event to SecurityHubFinding as {sec_hub_finding}")
                suppression_engine.cache_eligible_for_suppression_finding(
                    finding=sec_hub_finding,
                    event_type="SecurityHub"
                )

            # --- Case 3: Scheduled rule ---
            elif scheduler_type:
              if scheduler_type == 'historic-suppressed':
                logger.info("Event type : Historic Re-evaluate suppressed findings")
                suppression_engine.reevaluate_historic_suppressed_findings()
              elif scheduler_type == 'historic-new':
                logger.info("Event type : Historic Re-evaluate new findings")
                suppression_engine.reevaluate_historic_new_findings(context)
                suppression_engine.rotate_supported_regions()
              elif scheduler_type == 'current':
                logger.info("Event type : Suppress existing findings")
                suppression_engine.process_findings_from_cache()
                suppression_engine.process_findings_from_cache_for_deleted_rules()
            else:
              logger.warning("Unknown event type received")
